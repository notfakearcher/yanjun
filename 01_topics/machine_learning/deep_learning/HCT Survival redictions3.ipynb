{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HCT Survival Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:  Develop models to improve the prediction of transplant survival rates for patients undergoing allogeneic Hematopoietic Cell Transplantation (HCT) â€” an important step in ensuring that every patient has a fair chance at a successful outcome, regardless of their background.\n",
    "\n",
    "Improving survival predictions for allogeneic HCT patients is a vital healthcare challenge. Current predictive models often fall short in addressing disparities related to socioeconomic status, race, and geography. Addressing these gaps is crucial for enhancing patient care, optimizing resource utilization, and rebuilding trust in the healthcare system.\n",
    "\n",
    "The goal is to address disparities by bridging diverse data sources, refining algorithms, and reducing biases to ensure equitable outcomes for patients across diverse race groups. Your work will help create a more just and effective healthcare environment, ensuring every patient receives the care they deserve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Description\n",
    "\n",
    "The dataset consists of 59 variables related to hematopoietic stem cell transplantation (HSCT), encompassing a range of demographic and medical characteristics of both recipients and donors, such as age, sex, ethnicity, disease status, and treatment details. The primary outcome of interest is event-free survival, represented by the variable efs, while the time to event-free survival is captured by the variable efs_time. These two variables together encode the target for a censored time-to-event analysis. The data, which features equal representation across recipient racial categories including White, Asian, African-American, Native American, Pacific Islander, and More than One Race, was synthetically generated using the data generator from synthcity, trained on a large cohort of real CIBMTR data.\n",
    "\n",
    "\n",
    "    train.csv - the training set, with target efs (Event-free survival)\n",
    "    test.csv - the test set; your task is to predict the value of efs for this data\n",
    "    sample_submission.csv - a sample submission file in the correct format with all predictions set to 0.50\n",
    "    data_dictionary.csv - a list of all features and targets used in dataset and their descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data_dictionary = \"C:/Users/julia/Desktop/Yanjun/hct competition/data_dictionary.csv\"\n",
    "# path_test = \"C:/Users/julia/Desktop/Yanjun/hct competition/test.csv\"\n",
    "path_train = \"C:/Users/julia/Desktop/Yanjun/hct competition/train.csv\"\n",
    "# data_dictionary = pd.read_csv(path_data_dictionary)\n",
    "# path_submission = \"C:/Users/julia/Desktop/Yanjun/hct competition/sample_submission.csv\"\n",
    "# test = pd.read_csv(path_test)\n",
    "train = pd.read_csv(path_train)\n",
    "# submission = pd.read_csv(path_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Id from train\n",
    "train = train.drop(columns = 'ID')\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the correlation of missing data\n",
    "import missingno as msno\n",
    "msno.heatmap(train)\n",
    "# from the plot, i can see some missing variables are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the proposition of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = np.round(train.isna().sum()/len(train), 3) * 100\n",
    "df_missing = pd.DataFrame(missing, columns=['values']).sort_values(by = 'values', ascending = True)\n",
    "# mark different variables which has different category of missing data percentage:\n",
    "\n",
    "# function to differentiate different category percentage of missing data\n",
    "def color_map(percent):\n",
    "  cmap = []\n",
    "  for x in percent:\n",
    "    if x >= 20:\n",
    "      temp = 'background-color: red'\n",
    "    elif x >= 5:\n",
    "      temp = 'background-color: orange'\n",
    "    elif x >= 1:\n",
    "      temp = 'background-color: yellow'\n",
    "    else:\n",
    "      temp = 'background-color: green'\n",
    "    cmap.append(temp)\n",
    "  return cmap\n",
    "# df_missing.style.map(color_map)\n",
    "df_missing.style.apply(lambda x: color_map(df_missing['values']), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del missing, df_missing\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the missing data from  train and assence the importance of variables using randomforestsurvival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First , Use clean data to find the important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data = train.copy()\n",
    "# clean_data = clean_data.dropna()\n",
    "# len(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "# use clean data find variables importance\n",
    "\n",
    "# # change category variable to numerical variables in clean data\n",
    "# clean_data = pd.get_dummies(data = clean_data, drop_first= True, dtype = int)\n",
    "# # first baance the clean_data\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# smote = SMOTE(sampling_strategy = 'auto', random_state = 1)\n",
    "# cond = clean_data.columns == 'efs'\n",
    "# X_cond = clean_data.columns[~cond]\n",
    "# X, y = smote.fit_resample(clean_data[X_cond], clean_data['efs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_clean = pd.concat([X, y], axis = 1)\n",
    "# del X, y, clean_data, cond, smote\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.util import Surv\n",
    "# use this new clean dataset to get the important variables \n",
    "# rsf = RandomSurvivalForest(n_estimators= 30, max_depth= 20, max_features= 'sqrt', random_state= 1)\n",
    "# y = new_clean[['efs', 'efs_time']]\n",
    "# Y = Surv.from_dataframe('efs', 'efs_time', y)\n",
    "# cond = new_clean.columns.isin(['efs', 'efs_time'])\n",
    "# X = new_clean[new_clean.columns[~cond]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del new_clean\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsf.fit(X, Y)\n",
    "# rsf.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use permutation importance to calculate importance of features\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.inspection import permutation_importance\n",
    "# create a dataframe of feature importance\n",
    "# def C_score(estimator, X, y):\n",
    "#   y_pred = estimator.predict(X)\n",
    "#   return concordance_index_censored(Y['efs'], Y['efs_time'], y_pred)[0]\n",
    "  \n",
    "# feature_importance = permutation_importance(rsf, X = X, y = Y, scoring = C_score, n_repeats = 3, random_state = 1)\n",
    "# feature_importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_importance = feature_importance.importances_mean.mean()\n",
    "# index = np.where(feature_importance.importances_mean >= mean_importance)\n",
    "# import_variables_1 = X.columns[index]\n",
    "# import_variables_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these variables to fit the randomforestsurvival on original dataset, before doing this, clean old dataset, like X, Y, y\n",
    "# del X, Y, y, index, feature_importance, mean_importance\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first methods to find important variables is based on clean data, it is biased, so it need full data to use randomforestsurvival to find the important variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second , use full data (smaples ) and randomforestsurvival to find the important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance full orginal data\n",
    "\n",
    "\n",
    "from sklearn.utils import resample\n",
    "y_counts = train.efs.value_counts()\n",
    "minority = y_counts.index[np.where(y_counts != y_counts.max())].values[0]\n",
    "\n",
    "new = resample(train[train.efs == minority], replace = True, n_samples = (y_counts.max() - y_counts).max(), random_state = 1).reset_index(drop = True)\n",
    "train = pd.concat([train, new], axis = 0)\n",
    "train = train.reset_index(drop = True)\n",
    "train['efs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_counts, minority, new\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change category variables to numerical variables\n",
    "train1 = pd.get_dummies(train1, drop_first= True, dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "# define randomforestsurvival\n",
    "# randomsurvival = RandomSurvivalForest(n_estimators= 10, max_depth = 15, random_state= 1, max_features= 'sqrt')\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# # use small sample from train1 to calculate the important variables\n",
    "# small_train1 = train1.sample(frac= 0.05, random_state = 1)\n",
    "# cond = train1.columns.isin(['efs', 'efs_time'])\n",
    "# small_X = small_train1[small_train1.columns[~cond]]\n",
    "# small_y = train1.loc[small_X.index, ['efs', 'efs_time']]\n",
    "# small_y = Surv.from_dataframe('efs', 'efs_time',small_y )\n",
    "\n",
    "# del small_train1\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainydataframe = train1[['efs','efs_time']]\n",
    "# trainydataframe = Surv.from_dataframe('efs', 'efs_time', trainydataframe)\n",
    "# randomsurvival.fit(train1[train1.columns[~cond]],trainydataframe)\n",
    "# randomsurvival.score(train1[train1.columns[~cond]],trainydataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_feature = permutation_importance(estimator= randomsurvival, X = small_X, y = small_y,random_state= 1 , n_repeats = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del randomsurvival\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulid a dataframe for feature importance\n",
    "# importance = pd.DataFrame(data = importance_feature.importances_mean, index = small_X.columns,columns = ['importance1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del importance_feature\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del trainydataframe\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reorder variables accoring to the order of importance of variables\n",
    "# importance = importance.sort_values(by = 'importance1', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_ = importance.importance1.mean()\n",
    "# cond = importance.importance1 > mean_ * 0.3\n",
    "# important_variables = importance[cond].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del mean_, cond, small_X, small_y\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KNN to impute new dataset( important variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1= train1.astype(float)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing  import  StandardScaler\n",
    "scaler = StandardScaler()\n",
    "cond = train1.columns.isin(['efs', 'efs_time'])\n",
    "train1.loc[:,~cond] = scaler.fit_transform(train1.loc[:,~cond])\n",
    "\n",
    "# use knn to impute missing data\n",
    "imputer = KNNImputer(n_neighbors =  5)\n",
    "train1_complete = imputer.fit_transform(train1)\n",
    "train1_complete = pd.DataFrame(train1_complete, columns= train1.columns)\n",
    "train1_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del important_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine important_complete with y\n",
    "# important_complete['efs'] = train1['efs']\n",
    "# important_complete['efs_time'] = train1['efs_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove the outlier\n",
    "# from scipy import stats\n",
    "# z_scores = np.abs(stats.zscore(train1_complete))\n",
    "# threshold = 3\n",
    "# # train1_complete = train1_complete[(z_scores < threshold).all(axis = 1)]\n",
    "# train1_complete[(z_scores < threshold).all(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split important_complete into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_complete, test_complete = train_test_split(train1_complete, test_size= 0.3, random_state= 1, shuffle= True, stratify= train1_complete['efs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use randomforestsurvival to train the data\n",
    "cond = train_complete.columns.isin(['efs', 'efs_time'])\n",
    "\n",
    "train_y = Surv.from_dataframe('efs', 'efs_time', train_complete)\n",
    "test_y = Surv.from_dataframe('efs', 'efs_time', test_complete)\n",
    "RFS = RandomSurvivalForest(n_estimators= 15, random_state= 1, max_depth= 15, max_features= 'sqrt')\n",
    "\n",
    "# use model above to fit the data\n",
    "RFS.fit(train_complete[train_complete.columns[~cond]], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFS.score(train_complete[train_complete.columns[~cond]], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFS.score(test_complete[test_complete.columns[~cond]], test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "rfe = RFE(estimator=model)\n",
    "fit = rfe.fit(train_complete[train_complete.columns[~cond]], train_complete['efs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = rfe.ranking_\n",
    "# Sort the features by their ranking\n",
    "sorted_features = sorted(zip(train_complete.columns[~cond], ranking), key=lambda x: x[1])\n",
    "important_variables = []\n",
    "# Display the sorted features with their ranking\n",
    "print(\"Feature Importance (based on RFE ranking):\")\n",
    "for feature, rank in sorted_features:\n",
    "    if rank < 48:\n",
    "        important_variables.append(feature)\n",
    "# give me variables that ranking is less than 47(contain 47)\n",
    "important_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_features, ranking\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use batch processing to train randomsurvivalforest on important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first save new important variables and efs efs_time o train and test in new profile and delete old dataset \n",
    "important_variables.append('efs')\n",
    "important_variables.append('efs_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete[important_variables].to_csv(\"C:/Users/julia/Desktop/Yanjun/new_data.csv\", index = False)\n",
    "test_complete[important_variables].to_csv(\"C:/Users/julia/Desktop/Yanjun/new_test_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train1, train1_complete, train_complete, test_complete, important_variables, rfe, fit, model, RFS, train_y, test_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chunks to train randomssurvivalforest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(\"C:/Users/julia/Desktop/Yanjun/new_data.csv\", chunksize = 1000 )\n",
    "\n",
    "# define a function which can change dataframe to surv in chunks\n",
    "\n",
    "def surv_y_dataframe(chunk):\n",
    "  cond = chunk.columns.isin(['efs', 'efs_time'])\n",
    "  y_chunk = Surv.from_dataframe('efs', 'efs_time', chunk)\n",
    "  x_chunk = chunk[chunk.columns[~cond]]\n",
    "  return x_chunk, y_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random survival forest \n",
    "estmators = 100\n",
    "RSF = RandomSurvivalForest(n_estimators= estmators,  random_state= 1, warm_start= True, max_depth= 15, min_samples_split= 20, min_samples_leaf= 10,  max_features= 'sqrt')\n",
    "\n",
    "# train the first chunk on randomforest\n",
    "first_chunk = next(chunks)\n",
    "x_first_chunk, y_first_chunk = surv_y_dataframe(first_chunk)\n",
    "\n",
    "RSF.fit(x_first_chunk, y_first_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSF.score(x_first_chunk, y_first_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the number of trees increamentally with batch update\n",
    "for chunk in chunks:\n",
    "  x_chunk, y_chunk = surv_y_dataframe(chunk)\n",
    "  estmators = estmators + 10\n",
    "  RSF.n_estimators = estmators\n",
    "  \n",
    "  # fit update model with new chunk\n",
    "  RSF.fit(x_chunk, y_chunk)\n",
    "  print(RSF.score(x_chunk, y_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/julia/Desktop/Yanjun/new_test_data.csv\")\n",
    "x_test, y_test = surv_y_dataframe(test)\n",
    "RSF.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythontest2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
